{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bert using tranformer architecture(mentioned in paper attention is all you need (google)), uses contextual embedding, word in a sentents might have different meaning based on context\n",
    "* Transformer - It feeds input at once , compared to seq2seq feeding. \n",
    "* Bert is trained on mask language modelling & other is next sentence prediction\n",
    "* Eg - mask sentence prediction - thats [mask] she [mask] - > thats what she said\n",
    "* we are not using Bert giving in original paper .\n",
    "* we are using hugging face library , that provides pretrained models , we would be using pytorch version\n",
    "* Bert model is basically a stack of tranformer encoders \n",
    "* bert large & bert base have different number of encoders - large contains 24 tranformer , & base contains 12 tranformers\n",
    "* it uses attension mechanision, eg - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
